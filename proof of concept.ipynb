{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model use example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'grey parrot'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Function to prepare images for input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"grey.jpeg\")\n",
    "input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "    \n",
    "def predict_label(model, input_tensor):\n",
    "    url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "    labels = requests.get(url).json()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_label = labels[predicted.item()]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "predict_label(model, input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Attack Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proccessing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "def proccess_image(image_path: str) -> torch.Tensor:\n",
    "    \n",
    "    # Function to prepare images for input\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load an image\n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return input_tensor\n",
    "\n",
    "# Test\n",
    "image_path = \"grey.jpeg\"\n",
    "output = proccess_image(image_path)\n",
    "print(type(output))\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, make labels an attribute in the class so they're not a floating global variable\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "labels = requests.get(url).json()\n",
    "\n",
    "# return idx of target\n",
    "def get_target_idx(attack_target: str) -> int:\n",
    "    idx = labels.index(attack_target)\n",
    "    out = torch.tensor(idx).reshape(1)\n",
    "    return out \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3940, -1.0972],\n",
       "        [-0.2371, -0.0168]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.tensor([[1,2],[1,2]])\n",
    "t.randn(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: make training config and give user easier control over it\n",
    "\n",
    "def calculate_noise(image_data: torch.Tensor, attack_target: str) -> torch.Tensor:\n",
    "    \n",
    "    # intialize model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # get index of target label\n",
    "    target_idx = get_target_idx(attack_target)\n",
    "    \n",
    "    # initialize noise\n",
    "    noise = t.randn(image_data.shape)\n",
    "    noise.requires_grad = True\n",
    "    \n",
    "    optimizer = t.optim.AdamW([noise], lr=0.01)\n",
    "    \n",
    "    l1_lambda = 0.1\n",
    "    \n",
    "    # train noise\n",
    "    for step in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(image_data + noise)\n",
    "        \n",
    "        l1_norm = noise.abs().sum()\n",
    "        \n",
    "        loss = F.cross_entropy(logits, target_idx) + l1_lambda * l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f'step: {step}, loss: {loss}')\n",
    "            \n",
    "    print(predict_label(model, image_data + noise))        \n",
    "            \n",
    "    return noise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate and save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the preprocessing transformations\n",
    "def tensor_to_image(tensor: torch.Tensor) -> Image.Image:\n",
    "    # Remove the batch dimension if it exists\n",
    "    if tensor.ndimension() == 4:\n",
    "        tensor = tensor.squeeze(0)\n",
    "    \n",
    "    # # Unnormalize the image\n",
    "    # unnormalize = transforms.Normalize(\n",
    "    #     mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "    #     std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    "    # )\n",
    "    # unnormalized_tensor = unnormalize(tensor)\n",
    "    \n",
    "    # Clip the values to be between 0 and 1\n",
    "    unnormalized_tensor = torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    image = to_pil(unnormalized_tensor)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def generate_and_save_image(\n",
    "    image_data: torch.Tensor, \n",
    "    attack_noise: torch.Tensor, \n",
    "    save_path: str):\n",
    "\n",
    "    output_image = tensor_to_image(image_data + attack_noise)\n",
    "    output_image.show()  # Display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 11984.916015625\n",
      "step: 10, loss: 10538.59765625\n",
      "step: 20, loss: 9219.2880859375\n",
      "step: 30, loss: 8015.32666015625\n",
      "step: 40, loss: 6923.12646484375\n",
      "step: 50, loss: 5940.6396484375\n",
      "step: 60, loss: 5065.92236328125\n",
      "step: 70, loss: 4290.65869140625\n",
      "step: 80, loss: 3609.794921875\n",
      "step: 90, loss: 3016.499267578125\n",
      "step: 100, loss: 2504.61767578125\n",
      "step: 110, loss: 2065.1484375\n",
      "step: 120, loss: 1691.699462890625\n",
      "step: 130, loss: 1377.3438720703125\n",
      "step: 140, loss: 1113.34326171875\n",
      "step: 150, loss: 894.9472045898438\n",
      "step: 160, loss: 714.7329711914062\n",
      "step: 170, loss: 567.2591552734375\n",
      "step: 180, loss: 448.48828125\n",
      "step: 190, loss: 352.8793640136719\n",
      "step: 200, loss: 277.12384033203125\n",
      "step: 210, loss: 217.32179260253906\n",
      "step: 220, loss: 170.71145629882812\n",
      "step: 230, loss: 135.25726318359375\n",
      "step: 240, loss: 107.82251739501953\n",
      "step: 250, loss: 86.62982940673828\n",
      "step: 260, loss: 70.38602447509766\n",
      "step: 270, loss: 58.58849334716797\n",
      "step: 280, loss: 49.7969970703125\n",
      "step: 290, loss: 43.286869049072266\n",
      "step: 300, loss: 38.520084381103516\n",
      "step: 310, loss: 35.14585494995117\n",
      "step: 320, loss: 32.725975036621094\n",
      "step: 330, loss: 31.02231788635254\n",
      "step: 340, loss: 29.72260284423828\n",
      "step: 350, loss: 28.767288208007812\n",
      "step: 360, loss: 28.179248809814453\n",
      "step: 370, loss: 27.69594383239746\n",
      "step: 380, loss: 27.362930297851562\n",
      "step: 390, loss: 27.18170928955078\n",
      "step: 400, loss: 26.938512802124023\n",
      "step: 410, loss: 26.839736938476562\n",
      "step: 420, loss: 26.767011642456055\n",
      "step: 430, loss: 26.694194793701172\n",
      "step: 440, loss: 26.592660903930664\n",
      "step: 450, loss: 26.554363250732422\n",
      "step: 460, loss: 26.502450942993164\n",
      "step: 470, loss: 26.484722137451172\n",
      "step: 480, loss: 26.49561309814453\n",
      "step: 490, loss: 26.489227294921875\n",
      "step: 500, loss: 26.509033203125\n",
      "step: 510, loss: 26.499563217163086\n",
      "step: 520, loss: 26.45563507080078\n",
      "step: 530, loss: 26.461074829101562\n",
      "step: 540, loss: 26.41862678527832\n",
      "step: 550, loss: 26.40655517578125\n",
      "step: 560, loss: 26.37112808227539\n",
      "step: 570, loss: 26.406429290771484\n",
      "step: 580, loss: 26.436479568481445\n",
      "step: 590, loss: 26.416595458984375\n",
      "step: 600, loss: 26.429302215576172\n",
      "step: 610, loss: 26.324169158935547\n",
      "step: 620, loss: 26.37284278869629\n",
      "step: 630, loss: 26.41339111328125\n",
      "step: 640, loss: 26.42841911315918\n",
      "step: 650, loss: 26.3493709564209\n",
      "step: 660, loss: 26.401866912841797\n",
      "step: 670, loss: 26.392486572265625\n",
      "step: 680, loss: 26.47915267944336\n",
      "step: 690, loss: 26.42879867553711\n",
      "step: 700, loss: 26.416481018066406\n",
      "step: 710, loss: 26.398090362548828\n",
      "step: 720, loss: 26.387920379638672\n",
      "step: 730, loss: 26.346466064453125\n",
      "step: 740, loss: 26.395431518554688\n",
      "step: 750, loss: 26.389999389648438\n",
      "step: 760, loss: 26.38945770263672\n",
      "step: 770, loss: 26.37950897216797\n",
      "step: 780, loss: 26.27448272705078\n",
      "step: 790, loss: 26.340900421142578\n",
      "step: 800, loss: 26.425439834594727\n",
      "step: 810, loss: 26.37782859802246\n",
      "step: 820, loss: 26.311206817626953\n",
      "step: 830, loss: 26.36370086669922\n",
      "step: 840, loss: 26.266868591308594\n",
      "step: 850, loss: 26.325557708740234\n",
      "step: 860, loss: 26.365095138549805\n",
      "step: 870, loss: 26.35498046875\n",
      "step: 880, loss: 26.34581756591797\n",
      "step: 890, loss: 26.375537872314453\n",
      "step: 900, loss: 26.344051361083984\n",
      "step: 910, loss: 26.347360610961914\n",
      "step: 920, loss: 26.338478088378906\n",
      "step: 930, loss: 26.378738403320312\n",
      "step: 940, loss: 26.34982681274414\n",
      "step: 950, loss: 26.343740463256836\n",
      "step: 960, loss: 26.291236877441406\n",
      "step: 970, loss: 26.341094970703125\n",
      "step: 980, loss: 26.311887741088867\n",
      "step: 990, loss: 26.34099006652832\n",
      "grey parrot\n"
     ]
    }
   ],
   "source": [
    "def attack_image(\n",
    "    image_path: str,\n",
    "    save_path: str,\n",
    "    attack_target: str, \n",
    "    attack_method: str\n",
    "    ):\n",
    "    \n",
    "    # proccess image for resnet18\n",
    "    image_data = proccess_image(image_path)    \n",
    "    \n",
    "    # calculate noise which minimizes loss with respect to target output\n",
    "    attack_noise = calculate_noise(image_data, attack_target)\n",
    "    \n",
    "    # generate and save output image\n",
    "    generate_and_save_image(image_data, 0, \"hello\")\n",
    "    generate_and_save_image(image_data, attack_noise, save_path)\n",
    "    \n",
    "attack_image(\"grey.jpeg\", \"attacked_grey.jpeg\", \"macaw\", \"minimize_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
